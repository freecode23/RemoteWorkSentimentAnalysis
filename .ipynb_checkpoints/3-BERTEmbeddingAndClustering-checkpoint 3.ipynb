{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, manifold\n",
    "\n",
    "## for pre-processing\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## for language detection\n",
    "import langdetect\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "## for w2v\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "## for bert\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "  \n",
    "\n",
    "## for predicting\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re, nltk\n",
    "import my_functions as func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweetBERT.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "func.print_tweet(df, len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unidecode import unidecode\n",
    "\n",
    "def clean_BERT(text, isSentenceEmbed = True):\n",
    "\n",
    "    ''' Pre process and convert texts to a list of words \n",
    "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
    "\n",
    "    if(isSentenceEmbed == True):\n",
    "        # sentence embedding, we need hashatag\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!-?%.\\/#'+]\", \" \", text)\n",
    "    else:\n",
    "        # word embedding remove hashtag symbol\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!?%.\\/'+]\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" plus \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tweet = df.tweet.apply(lambda x: clean_BERT(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create new df that we need to create our word dictionary \n",
    "df_word = df.copy()[0:200]\n",
    "df_word = func.remove_end_hashtag(df_word)\n",
    "df_word.tweet = df.tweet.apply(lambda x: clean_BERT(x, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "func.print_tweet(df_word,len(df_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df_word = df_word.drop_duplicates(subset=['tweet'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DistilBERT Word Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>Create word dictionary<span>\n",
    "\n",
    "https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca<br>\n",
    "    https://medium.com/analytics-vidhya/bert-word-embeddings-deep-dive-32f6214f02bf<br>\n",
    "https://dzlab.github.io/dltips/en/tensorflow/create-bert-vocab/<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load and Test DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# 1. Instantiate DistilBERT tokenizer...we use the Fast version to optimize runtime\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', pad_token='[PAD]', model_max_length = 80)\n",
    "\n",
    "# 2. get word embedder (encoder)\n",
    "model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased',output_hidden_states=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = 'Dear Line Managers Appraisal your subordinate based on their Job performance and not sentiment blood-line religious group or tribe'\n",
    "\n",
    "# 1. encode\n",
    "ids = tokenizer.encode(txt, max_length = 75,pad_to_max_length=True,add_special_tokens = True, truncation=True)\n",
    "print(\"\\nIDs   :\\n\", ids)\n",
    "\n",
    "# 2. tokenize with CLS and SEP\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(\"\\nTokens with special:\\n\", tokens)\n",
    "\n",
    "# 3. Display the words with their IDs.\n",
    "for tup in zip(tokens, ids ):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will convert our sentence to 1 vector of 768D and store the result vector to our list\n",
    "# embedded_text_list = []\n",
    "# for i in range(5):\n",
    "#     text = df.iloc[i].tweet\n",
    "    \n",
    "#     #ids\n",
    "#     ids = tokenizer.encode(text)\n",
    "    \n",
    "#     #tokens\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "#     #array\n",
    "#     ids_arr = np.array(ids)[None,:]\n",
    "    \n",
    "#     #embed\n",
    "#     embedding = model(ids_arr)\n",
    "    \n",
    "#     embedded_text_list.append(embedding)\n",
    "# # we have 5 samples x 768 \n",
    "\n",
    "# # total number of tweet\n",
    "# print(len(embedded_text_list))\n",
    "\n",
    "# # shape of each embedded tweet\n",
    "# print(embedded_text_list[1][0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Tokenize and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1 Encoding and Tokenize Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>We first need to get the maximum length of the tweet so we can pad it properly<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_np = df_word.tweet.apply(lambda x: tokenizer.encode(x))\n",
    "max_len = 0\n",
    "for ids in ids_np.values:\n",
    "    if len(ids) > max_len:\n",
    "        max_len = len(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>We want our IDs numpy to be in this format<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get ids as numpy array\n",
    "id_ = df_word.tweet.apply(lambda x: tokenizer.encode(x, add_special_tokens = True)) # Add [PAD]s\n",
    "\n",
    "idJ_np = np.array([i + [0]*(max_len-len(i)) for i in id_.values])\n",
    "\n",
    "print('\\nshape:\\n', idJ_np.shape)\n",
    "print(idJ_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_maskJ = np.where(idJ_np != 0, 1, 0)\n",
    "print('\\nshape:\\n',attention_maskJ.shape)\n",
    "print(attention_maskJ[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Encoding to ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Encoding\n",
    "ids_attn = df_word.tweet.apply(lambda x: tokenizer.encode_plus(x,\n",
    "                                                        add_special_tokens = True,\n",
    "                                                        max_length =max_len,# maximum length of a sentence\n",
    "                                                        truncation=True,\n",
    "                                                        return_tensors = 'np',\n",
    "                                                        padding = 'max_length')) # Add [PAD]s\n",
    "print(ids_attn)\n",
    "# print(type(ids_attn[0]['input_ids']))   \n",
    "# print(ids_attn[0]['attention_mask'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get IDs numpy\n",
    "input_ids_list = []\n",
    "for i in range(len(ids_attn)):\n",
    "    input_ids_list.append(ids_attn[i]['input_ids'][0])\n",
    "\n",
    "    \n",
    "input_ids_np = np.array(input_ids_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Encoding to Attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Attention_mask numpy\n",
    "\n",
    "attention_mask_list = []\n",
    "for i in range(len(ids_attn)):\n",
    "    attention_mask_list.append(ids_attn[i]['attention_mask'][0])\n",
    "    \n",
    "    \n",
    "attention_mask_np = np.array(attention_mask_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the index match 39 is [SEP] and so 40 is [PAD]\n",
    "print(\"input id's shape: \", input_ids_np.shape)\n",
    "print(\"attention mask's shape: \", attention_mask_np.shape)\n",
    "\n",
    "print('\\n',input_ids_np[0])\n",
    "print('\\nAt index 39 input id is: ',input_ids_np[0][39])\n",
    "print('At index 40 input id is: ',input_ids_np[0][40])\n",
    "print('\\nAt Index 39 attention is: ', attention_mask_np[0][39])\n",
    "print('At Index 40 attention is: ',attention_mask_np[0][40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Tokenize to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_np = df_word.tweet.apply(lambda x: tokenizer.tokenize(x,\n",
    "                                                             add_special_tokens = True,\n",
    "                                                             max_length = max_len,\n",
    "                                                             truncation=True,\n",
    "                                                             return_tensors = 'np',\n",
    "                                                             pad_to_max_length=True))\n",
    "tokens_np[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save IDs, attention_mask, and tokens as DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.1 Save tokens in Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df = pd.DataFrame(tokens_np)\n",
    "tokens_df.rename(columns = {'tweet': 'tweet_tokens'})\n",
    "df_word['tokens'] = tokens_df\n",
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Save Ids as Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assign empty list to token ids column: of shape (len(df_word) = 100, max_len = 67 words)\n",
    "df_word['token_ids'] = np.empty((len(df_word), max_len)).tolist()\n",
    "\n",
    "# assign token ids \n",
    "df_word['token_ids'] = input_ids_list\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Save attention mask as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word['attention_mask'] = np.empty((len(df_word),max_len)).tolist()\n",
    "\n",
    "df_word['attention_mask'] = attention_mask_list\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_word.iloc[3].tokens)\n",
    "# print(df_word.iloc[3].attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedded last hidden states\n",
    "last_hidden_states = model(input_ids_np, attention_mask=attention_mask_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 7. Check our last hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](last_hidden.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'> first  [0] is the Tensor <br>\n",
    "second [0] is the tweet <br>\n",
    "third [0] is the word  <br><span>\n",
    "fourth [0] is the part of tensor of 768  <br><span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the embedding of the last word(PAD) of 1st and 2nd sentence\n",
    "print(last_hidden_states[0][0,63,:5])\n",
    "print(last_hidden_states[0][1,63,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets check the embedding for the word con in the first tweet\n",
    "con_embed = last_hidden_states[0][1,1]\n",
    "print(type(con_embed))\n",
    "\n",
    "con_embed_np = np.array(last_hidden_states[0][1,1])\n",
    "print(type(con_embed_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(con_embed_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8. Test Get embedding for each word in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_idx = 0\n",
    "tokens_tweets = df_word.iloc[sentence_idx].tokens\n",
    "embedding_tweets = last_hidden_states[0][sentence_idx]\n",
    "\n",
    "print(tokens_tweets)\n",
    "print(embedding_tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Get Words Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_word_df(the_tokens_list):   \n",
    "#     # Part A: words df\n",
    "#     the_clean_tokens_list = []\n",
    "#     the_tokens_idx = []\n",
    "#     merged_tokens_idx = []\n",
    "#     max_len = len(the_tokens_list)\n",
    "#     i = 0\n",
    "#     word = the_tokens_list[i]\n",
    "#     while(i < max_len):\n",
    "#         add = True\n",
    "#         word = the_tokens_list[i]\n",
    "#         # 1. join ## words first\n",
    "#         # if its not the last word, \n",
    "#         if(i <= (max_len-2)):\n",
    "#             next_word = the_tokens_list[i+1]\n",
    "#             pattern = r\"(^##)\"\n",
    "#             # if the next word is ##\n",
    "#             while (bool(re.search(pattern,next_word))):\n",
    "#                 add = False\n",
    "#                 print('index of word with ##: ',i+1)\n",
    "#                 word = word + next_word[2:]\n",
    "#                 if(i) not in merged_tokens_idx:\n",
    "#                     merged_tokens_idx.append(i)\n",
    "#                 merged_tokens_idx.append(i+1)\n",
    "#                 print('word after merge:', word)\n",
    "#                 # skip the next word becos we have merged it\n",
    "#                 i+=1\n",
    "#                 next_word = the_tokens_list[i+1]\n",
    "        \n",
    "#         if(merged_tokens_idx) not in the_tokens_idx:\n",
    "#             the_tokens_idx.append(merged_tokens_idx) \n",
    "#         # 2. get tokens that are not padding, CLS, PAD, SEP, and not in stopwords\n",
    "#         if(word!= '[CLS]') and (word!= '[PAD]') and (word!= '[SEP]') and (word not in stopwords.words('english')):     \n",
    "#             the_clean_tokens_list.append(word)\n",
    "#             if(add == True):\n",
    "#                 the_tokens_idx.append(i)\n",
    "                \n",
    "        \n",
    "#         # 3. move to next word\n",
    "#         i+=1\n",
    "    \n",
    "#     the_df_dict= pd.DataFrame(the_clean_tokens_list)\n",
    "#     the_df_dict.columns = ['words']\n",
    "#     print(merged_tokens_idx)\n",
    "#     return [the_df_dict ,the_tokens_idx, the_clean_tokens_list]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_df(the_tokens_list):   \n",
    "    the_clean_tokens_list= []\n",
    "    the_tokens_idx = []\n",
    "    i = 0\n",
    "    \n",
    "    # 1. loop through the token list\n",
    "    word = the_tokens_list[i]\n",
    "    for i in range(len(the_tokens_list)):\n",
    "        word = the_tokens_list[i]\n",
    "        # 2. If its not specialised token and words are not in stop words:\n",
    "        if(word!= '[CLS]') and (word!= '[PAD]') and (word!= '[SEP]') and (word not in stopwords.words('english')):     \n",
    "            the_clean_tokens_list.append(word)\n",
    "            the_tokens_idx.append(i)     \n",
    "\n",
    "\n",
    "    the_df_dict= pd.DataFrame(the_clean_tokens_list)\n",
    "    the_df_dict.columns = ['words']\n",
    "    return [the_df_dict ,the_tokens_idx, the_clean_tokens_list]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict,tokens_idx,clean_tokens_list  = create_word_df(tokens_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Get Embedding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_df(the_df_dict,the_embeddings_list, the_idx_list):\n",
    "    clean_embedding_list = []\n",
    "    \n",
    "    # 1. loop through tokens_idx \n",
    "    for i in range(len(the_idx_list)):\n",
    "        embed_idx = the_idx_list[i]\n",
    "        \n",
    "        # print(np.array(the_embeddings_list[embed_idx][:5]))\n",
    "        # get the embedding from embedding_list\n",
    "        clean_embedding_list.append(np.array(the_embeddings_list[embed_idx]))\n",
    "             \n",
    "    # 2. numpy form \n",
    "    the_embeddings_np = np.array(clean_embedding_list)\n",
    "    \n",
    "    # make embeddings:\n",
    "    # 3. df form\n",
    "    the_df_dict['embeddings'] = np.empty([len(the_idx_list), 768]).tolist()\n",
    "#     print(np.array(the_embeddings_list[0]))\n",
    "#     print('dfdict shape: ', the_df_dict['embeddings'].shape)\n",
    "    \n",
    "    # print(the_df_dict)\n",
    "    the_df_dict['embeddings'] = clean_embedding_list\n",
    "    \n",
    "    return [the_df_dict, the_embeddings_np]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dict, embeddings_np = create_embedding_df(df_dict, embedding_tweets, tokens_idx)\n",
    "df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Get word and embedding for all tweets Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_words_and_embedding_from_df(the_df):\n",
    "    \n",
    "    tokens_list = the_df.iloc[0].tokens\n",
    "    embedding_list = last_hidden_states[0][0]\n",
    "    \n",
    "    final_df_dict, tokens_idx,clean_tokens_list = create_word_df(tokens_tweets)\n",
    "    final_df_dict, the_embeddings_np = create_embedding_df(final_df_dict, embedding_list, tokens_idx)\n",
    "    final_df_dict['sentence_idx'] = 0\n",
    "    \n",
    "    #for rest of each tweet/ row\n",
    "    for sentence_idx in range(1,len(the_df)):\n",
    "        # print(sentence_idx)\n",
    "        # 1. get list of tokens/words in a tweet\n",
    "        tokens_list = the_df.iloc[sentence_idx].tokens\n",
    "        \n",
    "        # 2. get list of embedding in a tweet\n",
    "        embedding_list = last_hidden_states[0][sentence_idx]\n",
    "        \n",
    "        # 3. get word df\n",
    "        df_dict, tokens_idx,clean_tokens_list = create_word_df(tokens_list)\n",
    "        df_dict['sentence_idx'] = int(sentence_idx)\n",
    "        \n",
    "        #4. add embeddings columns to the df\n",
    "        df_dict, the_embeddings_np = create_embedding_df(df_dict, embedding_list, tokens_idx)\n",
    "        \n",
    "        \n",
    "        frames = [final_df_dict, df_dict]\n",
    "\n",
    "        final_df_dict = pd.concat(frames)\n",
    "        \n",
    "        \n",
    "    return final_df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = get_words_and_embedding_from_df(df_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict.reset_index(drop = True, inplace = True)\n",
    "df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Double check if the embedding created is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 16th word of the last sentence tweets from df_words\n",
    "print(df_word.iloc[999].tokens[52])\n",
    "\n",
    "# get the embedding: [tensor][sentence][words][nth value]\n",
    "last_hidden_states[0][999][52][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Convert our embedding column to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Pandas series to numpy array\n",
    "# convert first to list, then convert to array\n",
    "embeddings_np = np.array(df_dict.embeddings.tolist())\n",
    "embeddings_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question. There are no duplicates, its normal?\n",
    "embeddings_list = [tuple(row) for row in embeddings_np]\n",
    "embeddings_unique_np, unique_index = np.unique(embeddings_list,axis = 0, return_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_unique_np.shape)\n",
    "unique_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>There are 22,071 duplicates. Lets Remove them from df_dict<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. convert the numpy array to tupple. \n",
    "# 2. apply duplicated() to the tuple\n",
    "# 3. get the duplicated embeddings\n",
    "# 4. slice where its not duplicated\n",
    "df_dict = df_dict[~df_dict['embeddings'].apply(tuple).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 9. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'> We want to make sure that documents with similar sentiments are clustered together such that we can find the topics within these clusters. Before doing so, we first need to lower the dimensionality of the embeddings as many clustering algorithms handle high dimensionality poorly. <span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9.1 Reduce Dimensionality using UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\">Out of the few dimensionality reduction algorithms, UMAP is arguably the best performing as it keeps a significant portion of the high-dimensional local structure in lower dimensionality.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_unique_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_reduced_np = umap.UMAP(n_neighbors=15,\n",
    "                       n_components=100,\n",
    "                       metric='cosine').fit_transform(embeddings_unique_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_reduced_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9.2 Clustering KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\"> After having reduced the dimensionality of the documents embeddings to 5, we can cluster the documents with HDBSCAN. HDBSCAN is a density-based algorithm that works quite well with UMAP since UMAP maintains a lot of local structure even in lower-dimensional space. Moreover, HDBSCAN does not force data points to clusters as it considers them outliers.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install hdbscan --no-build-isolation --no-binary :all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KMeans = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50)\n",
    "labels = model_KMeans.fit_predict(X=embeddings_reduced_np)\n",
    "# positive_cluster_center = model_KMeans.cluster_centers_[0]\n",
    "# negative_cluster_center = model_KMeans.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame(labels, columns =['cluster_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['cluster_label'] = cluster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_0 = df_dict[df_dict['cluster_label'] == 0].words.reset_index(drop = True)\n",
    "dict_1 = df_dict[df_dict['cluster_label'] == 1].words.reset_index(drop = True)\n",
    "\n",
    "\n",
    "print('cluster 0 counts : ',len(dict_0))\n",
    "print('cluster 1 counts : ',len(dict_1))\n",
    "\n",
    "for i in range(len(dict_1)):\n",
    "    print(dict_1.iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(dict_0)):\n",
    "    print(dict_0.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(dict_1)):\n",
    "    print(dict_1.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"background-color:Teal\">Question: <span><br>\n",
    "<span style=\"background-color:Teal\">- There is a mix of postiive and negative words. fever is negative while happy is positive <br>\n",
    "<span style=\"background-color:Teal\">- Subwords are separated from their original word <br><span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 9.3 Get top 20 words that are of closest distance from centroids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\">- Question: how to do this? the article I saw use gensim similarity<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\">- Question:How to find distance between centroids and words<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distance = model_KMeans.fit_transform(X=embeddings_reduced_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDistance = np.min(all_distance, axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4 HDBScan Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = hdbscan.HDBSCAN(min_cluster_size=2,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(embeddings_reduced_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(np.unique(cluster.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"background-color:Teal\">Question: <span><br>\n",
    "<span style=\"background-color:Teal\">- Why are there so many cluster? This is not good for sentiment clustering<span>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
