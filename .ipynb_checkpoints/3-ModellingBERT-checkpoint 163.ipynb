{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, manifold\n",
    "\n",
    "## for pre-processing\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## for language detection\n",
    "import langdetect\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "## for w2v\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "## for bert\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "  \n",
    "\n",
    "## for predicting\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re, nltk\n",
    "import my_functions as func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweetBERT.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "func.print_tweet(df, len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unidecode import unidecode\n",
    "\n",
    "def clean_BERT(text, isSentenceEmbed = True):\n",
    "\n",
    "    ''' Pre process and convert texts to a list of words \n",
    "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
    "\n",
    "    if(isSentenceEmbed == True):\n",
    "        # sentence embedding, we need hashatag\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!-?%.\\/#'+]\", \" \", text)\n",
    "    else:\n",
    "        # word embedding remove hashtag symbol\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!?%.\\/'+]\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" plus \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tweet = df.tweet.apply(lambda x: clean_BERT(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create new df that we need to create our word dictionary \n",
    "df_word = df.copy()[0:100]\n",
    "df_word = func.remove_end_hashtag(df_word)\n",
    "df_word.tweet = df.tweet.apply(lambda x: clean_BERT(x, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "func.print_tweet(df_word,len(df_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df_word = df_word.drop_duplicates(subset=['tweet'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>Create word dictionary<span>\n",
    "\n",
    "https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca<br>\n",
    "    https://medium.com/analytics-vidhya/bert-word-embeddings-deep-dive-32f6214f02bf<br>\n",
    "https://dzlab.github.io/dltips/en/tensorflow/create-bert-vocab/<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Distil BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Load and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Instantiate DistilBERT tokenizer...we use the Fast version to optimize runtime\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', pad_token='[PAD]', model_max_length = 80)\n",
    "\n",
    "# 2. get word embedder (encoder)\n",
    "model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased',output_hidden_states=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to convert our id into 2D numpy array by using [None,:]\n",
    "# labels=np.array([1,3,5])\n",
    "# print('2D array rows: \\n',labels[None,:])\n",
    "# print('2D array cols: \\n',labels[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = 'Dear Line Managers Appraisal your subordinate based on their Job performance and not sentiment blood-line religious group or tribe'\n",
    "\n",
    "# 1. encode\n",
    "ids = tokenizer.encode(txt, padding = True)\n",
    "print(\"\\nIDs   :\\n\", ids)\n",
    "\n",
    "# 2. tokenize with CLS and SEP\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(\"\\nTokens with special:\\n\", tokens)\n",
    "\n",
    "# 3. Display the words with their IDs.\n",
    "for tup in zip(tokens, ids ):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will convert our sentence to 1 vector of 768D and store the result vector to our list\n",
    "# embedded_text_list = []\n",
    "# for i in range(5):\n",
    "#     text = df.iloc[i].tweet\n",
    "    \n",
    "#     #ids\n",
    "#     ids = tokenizer.encode(text)\n",
    "    \n",
    "#     #tokens\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "#     #array\n",
    "#     ids_arr = np.array(ids)[None,:]\n",
    "    \n",
    "#     #embed\n",
    "#     embedding = model(ids_arr)\n",
    "    \n",
    "#     embedded_text_list.append(embedding)\n",
    "# # we have 5 samples x 768 \n",
    "\n",
    "# # total number of tweet\n",
    "# print(len(embedded_text_list))\n",
    "\n",
    "# # shape of each embedded tweet\n",
    "# print(embedded_text_list[1][0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get max sentences length\n",
    "# max_len = 0\n",
    "# for i in ids_np.values:\n",
    "#     if len(i) > max_len:\n",
    "#         max_len = len(i)\n",
    "\n",
    "# max_len\n",
    "df_word.tweet.str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets try a small subset first\n",
    "ids_np = df_word.tweet.apply(lambda x: tokenizer.encode(x,\n",
    "                                                        add_special_tokens = True,\n",
    "#                                                         max_length = 75,# maximum length of a sentence\n",
    "#                                                         truncation=True,\n",
    "                                                        pad_to_max_length=True)) # Add [PAD]s\n",
    "ids_np[0]                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To tokens words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_np = df_word.tweet.apply(lambda x: tokenizer.tokenize(x,\n",
    "                                                             add_special_tokens = True,\n",
    "                                                             max_length = 75,\n",
    "                                                             truncation=True,\n",
    "                                                             pad_to_max_length=True))\n",
    "tokens_np[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Save IDs and Tokens as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df = pd.DataFrame(tokens_np)\n",
    "tokens_df.rename(columns = {'tweet': 'tweet_tokens'})\n",
    "df_word['tokens'] = tokens_df\n",
    "df_word = df_word[0:100]\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_list = id_np.tolist()\n",
    "\n",
    "# assign empty list to token ids column\n",
    "df_word['token_ids'] = np.empty((len(df), 74)).tolist()\n",
    "\n",
    "# assign token ids \n",
    "df_word['token_ids'] = id_list\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style ='background-color:Teal'>If we directly send padded to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(id_np != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. convert 'padded ids' and 'attention mask' to tensors\n",
    "# input_ids = torch.tensor(padded)  \n",
    "# attention_mask1 = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get embedded last hidden states\n",
    "last_hidden_states = model(id_np, attention_mask=attention_mask, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Get embedding for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one tweet, split the vector into 1*768 vector\n",
    "print(last_hidden_states[0][0][73].shape)\n",
    "print(last_hidden_states[0][0][73].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'> We want to make sure that documents with similar sentiments are clustered together such that we can find the topics within these clusters. Before doing so, we first need to lower the dimensionality of the embeddings as many clustering algorithms handle high dimensionality poorly. <span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1 Reduce Dimensionality using UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\">Out of the few dimensionality reduction algorithms, UMAP is arguably the best performing as it keeps a significant portion of the high-dimensional local structure in lower dimensionality.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap_tweet_list = []\n",
    "for embedded_tweet in embedded_text_list:\n",
    "    umap_tweet = umap.UMAP(n_neighbors=15,\n",
    "                           n_components=5,\n",
    "                           metric='cosine').fit_transform(embedded_tweet)\n",
    "    umap_tweet_list.append(umap_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of tweet\n",
    "print(len(umap_tweet_list))\n",
    "\n",
    "# shape of each embedded tweet\n",
    "print(umap_tweet_list[6268].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.2 Clustering KNN?\n",
    "Question : I see HDBSCAN, can I just use KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\"> After having reduced the dimensionality of the documents embeddings to 5, we can cluster the documents with HDBSCAN. HDBSCAN is a density-based algorithm that works quite well with UMAP since UMAP maintains a lot of local structure even in lower-dimensional space. Moreover, HDBSCAN does not force data points to clusters as it considers them outliers.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install hdbscan --no-build-isolation --no-binary :all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)\n",
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
