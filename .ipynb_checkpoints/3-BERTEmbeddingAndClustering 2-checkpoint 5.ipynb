{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, manifold\n",
    "\n",
    "## for pre-processing\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## for language detection\n",
    "import langdetect\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "## for w2v\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "## for bert\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "  \n",
    "\n",
    "## for predicting\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re, nltk\n",
    "import my_functions as func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweetBERT.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "func.print_tweet(df, len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unidecode import unidecode\n",
    "\n",
    "def clean_BERT(text, isSentenceEmbed = True):\n",
    "\n",
    "    ''' Pre process and convert texts to a list of words \n",
    "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
    "\n",
    "    if(isSentenceEmbed == True):\n",
    "        # sentence embedding, we need hashatag\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!-?%.\\/#'+]\", \" \", text)\n",
    "    else:\n",
    "        # word embedding remove hashtag symbol\n",
    "        text = re.sub(r\"[^A-Za-z0-9^]\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" plus \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tweet = df.tweet.apply(lambda x: clean_BERT(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "func.print_tweet(df,len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create new df that we need to create our word dictionary \n",
    "df_word = df.copy()[0:3000]\n",
    "df_word = func.remove_end_hashtag(df_word)\n",
    "df_word.tweet = df.tweet.apply(lambda x: clean_BERT(x, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "func.print_tweet(df_word,len(df_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df_word = df_word.drop_duplicates(subset=['tweet'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DistilBERT Word Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>Create word dictionary<span>\n",
    "\n",
    "https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca<br>\n",
    "    https://medium.com/analytics-vidhya/bert-word-embeddings-deep-dive-32f6214f02bf<br>\n",
    "https://dzlab.github.io/dltips/en/tensorflow/create-bert-vocab/<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load and Test DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# 1. Instantiate DistilBERT tokenizer...we use the Fast version to optimize runtime\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', pad_token='[PAD]', model_max_length = 87)\n",
    "\n",
    "# 2. get word embedder (encoder)\n",
    "model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased',output_hidden_states=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = 'Dear Line Managers Appraisal your subordinate based on their Job performance and not sentiment blood-line religious group or tribe'\n",
    "\n",
    "# 1. encode\n",
    "ids = tokenizer.encode(txt, max_length = 87,pad_to_max_length=True,add_special_tokens = True, truncation=True)\n",
    "print(\"\\nIDs   :\\n\", ids)\n",
    "\n",
    "# 2. tokenize with CLS and SEP\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(\"\\nTokens with special:\\n\", tokens)\n",
    "\n",
    "# 3. Display the words with their IDs.\n",
    "for tup in zip(tokens, ids ):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Tokenize and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1 Encoding and Tokenize Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>We first need to get the maximum length of the tweet so we can pad it properly<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_np = df_word.tweet.apply(lambda x: tokenizer.encode(x))\n",
    "max_len = 0\n",
    "for ids in ids_np.values:\n",
    "    if len(ids) > max_len:\n",
    "        max_len = len(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get ids as numpy array\n",
    "id_ = df_word.tweet.apply(lambda x: tokenizer.encode(x, add_special_tokens = True)) # Add [PAD]s\n",
    "\n",
    "idJ_np = np.array([i + [0]*(max_len-len(i)) for i in id_.values])\n",
    "\n",
    "print('\\nshape:\\n', idJ_np.shape)\n",
    "print(idJ_np[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>We want our IDs numpy to be in this format<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_maskJ = np.where(idJ_np != 0, 1, 0)\n",
    "print('\\nshape:\\n',attention_maskJ.shape)\n",
    "print(attention_maskJ[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Encoding to ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Encoding\n",
    "ids_attn = df_word.tweet.apply(lambda x: tokenizer.encode_plus(x,\n",
    "                                                        add_special_tokens = True,\n",
    "                                                        max_length =max_len,# maximum length of a sentence\n",
    "                                                        truncation=True,\n",
    "                                                        return_tensors = 'np',\n",
    "                                                        padding = 'max_length')) # Add [PAD]s\n",
    "print(ids_attn)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get IDs numpy\n",
    "input_ids_list = []\n",
    "for i in range(len(ids_attn)):\n",
    "    input_ids_list.append(ids_attn[i]['input_ids'][0])\n",
    "\n",
    "    \n",
    "input_ids_np = np.array(input_ids_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Encoding to Attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Attention_mask numpy\n",
    "\n",
    "attention_mask_list = []\n",
    "for i in range(len(ids_attn)):\n",
    "    attention_mask_list.append(ids_attn[i]['attention_mask'][0])\n",
    "    \n",
    "    \n",
    "attention_mask_np = np.array(attention_mask_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the index match 39 is [SEP] and so 40 is [PAD]\n",
    "print(\"input id's shape: \", input_ids_np.shape)\n",
    "print(\"attention mask's shape: \", attention_mask_np.shape)\n",
    "\n",
    "print('\\n',input_ids_np[0])\n",
    "print('\\nAt index 39 input id is: ',input_ids_np[0][38])\n",
    "print('At index 40 input id is: ',input_ids_np[0][39])\n",
    "print('\\nAt Index 39 attention is: ', attention_mask_np[0][38])\n",
    "print('At Index 40 attention is: ',attention_mask_np[0][39])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Tokenize to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_np = df_word.tweet.apply(lambda x: tokenizer.tokenize(x,\n",
    "                                                             add_special_tokens = True,\n",
    "                                                             max_length = max_len,\n",
    "                                                             truncation=True,\n",
    "                                                             return_tensors = 'np',\n",
    "                                                             pad_to_max_length=True))\n",
    "tokens_np[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save IDs, attention_mask, and tokens as DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.1 Save tokens in Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df = pd.DataFrame(tokens_np)\n",
    "tokens_df.rename(columns = {'tweet': 'tweet_tokens'})\n",
    "df_word['tokens'] = tokens_df\n",
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Save Token Ids as Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assign empty list to token ids column: of shape (len(df_word) = 100, max_len = 67 words)\n",
    "df_word['token_ids'] = np.empty((len(df_word), max_len)).tolist()\n",
    "\n",
    "# assign token ids \n",
    "df_word['token_ids'] = input_ids_list\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Save attention mask as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_word['attention_mask'] = np.empty((len(df_word),max_len)).tolist()\n",
    "\n",
    "df_word['attention_mask'] = attention_mask_list\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedded last hidden states\n",
    "last_hidden_states = model(input_ids_np, attention_mask=attention_mask_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 7. Check our last hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](last_hidden.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'> first  [0] is the Tensor <br>\n",
    "second [0] is the tweet <br>\n",
    "third [0] is the word  <br><span>\n",
    "fourth [0] is the part of tensor of 768  <br><span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets check the embedding for the word con in the first tweet\n",
    "con_embed = last_hidden_states[0][1,1]\n",
    "print(type(con_embed))\n",
    "\n",
    "con_embed_np = np.array(last_hidden_states[0][1,1])\n",
    "print(type(con_embed_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(con_embed_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8. Test Get embedding for each word in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_idx = 0\n",
    "tokens_tweets = df_word.iloc[sentence_idx].tokens\n",
    "embedding_tweets = last_hidden_states[0][sentence_idx]\n",
    "\n",
    "print(tokens_tweets)\n",
    "print(embedding_tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Get Words Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_word_df(the_tokens_list):   \n",
    "#     # Part A: words df\n",
    "#     the_clean_tokens_list = []\n",
    "#     the_tokens_idx = []\n",
    "#     merged_tokens_idx = []\n",
    "#     max_len = len(the_tokens_list)\n",
    "#     i = 0\n",
    "#     word = the_tokens_list[i]\n",
    "#     while(i < max_len):\n",
    "#         add = True\n",
    "#         word = the_tokens_list[i]\n",
    "#         # 1. join ## words first\n",
    "#         # if its not the last word, \n",
    "#         if(i <= (max_len-2)):\n",
    "#             next_word = the_tokens_list[i+1]\n",
    "#             pattern = r\"(^##)\"\n",
    "#             # if the next word is ##\n",
    "#             while (bool(re.search(pattern,next_word))):\n",
    "#                 add = False\n",
    "#                 print('index of word with ##: ',i+1)\n",
    "#                 word = word + next_word[2:]\n",
    "#                 if(i) not in merged_tokens_idx:\n",
    "#                     merged_tokens_idx.append(i)\n",
    "#                 merged_tokens_idx.append(i+1)\n",
    "#                 print('word after merge:', word)\n",
    "#                 # skip the next word becos we have merged it\n",
    "#                 i+=1\n",
    "#                 next_word = the_tokens_list[i+1]\n",
    "        \n",
    "#         if(merged_tokens_idx) not in the_tokens_idx:\n",
    "#             the_tokens_idx.append(merged_tokens_idx) \n",
    "#         # 2. get tokens that are not padding, CLS, PAD, SEP, and not in stopwords\n",
    "#         if(word!= '[CLS]') and (word!= '[PAD]') and (word!= '[SEP]') and (word not in stopwords.words('english')):     \n",
    "#             the_clean_tokens_list.append(word)\n",
    "#             if(add == True):\n",
    "#                 the_tokens_idx.append(i)\n",
    "                \n",
    "        \n",
    "#         # 3. move to next word\n",
    "#         i+=1\n",
    "    \n",
    "#     the_df_dict= pd.DataFrame(the_clean_tokens_list)\n",
    "#     the_df_dict.columns = ['words']\n",
    "#     print(merged_tokens_idx)\n",
    "#     return [the_df_dict ,the_tokens_idx, the_clean_tokens_list]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_df(the_tokens_list):   \n",
    "    the_clean_tokens_list= []\n",
    "    the_tokens_idx = []\n",
    "    i = 0\n",
    "    \n",
    "    # 1. loop through the token list\n",
    "    word = the_tokens_list[i]\n",
    "    for i in range(len(the_tokens_list)):\n",
    "        word = the_tokens_list[i]\n",
    "        # 2. If its not specialised token and words are not in stop words:\n",
    "        if(word!= '[CLS]') and (word!= '[PAD]') and (word!= '[SEP]') and (word not in stopwords.words('english')):     \n",
    "            the_clean_tokens_list.append(word)\n",
    "            the_tokens_idx.append(i)     \n",
    "\n",
    "\n",
    "    the_df_dict= pd.DataFrame(the_clean_tokens_list)\n",
    "    the_df_dict.columns = ['words']\n",
    "    return [the_df_dict ,the_tokens_idx, the_clean_tokens_list]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict,tokens_idx,clean_tokens_list  = create_word_df(tokens_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Get Embedding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_df(the_df_dict,the_embeddings_list, the_idx_list):\n",
    "    clean_embedding_list = []\n",
    "    \n",
    "    # 1. loop through tokens_idx \n",
    "    for i in range(len(the_idx_list)):\n",
    "        embed_idx = the_idx_list[i]\n",
    "\n",
    "        # get the embedding from embedding_list\n",
    "        clean_embedding_list.append(np.array(the_embeddings_list[embed_idx]))\n",
    "             \n",
    "    # 2. numpy form \n",
    "    the_embeddings_np = np.array(clean_embedding_list)\n",
    "    \n",
    "    # make embeddings:\n",
    "    # 3. df form\n",
    "    the_df_dict['embeddings'] = np.empty([len(the_idx_list), 768]).tolist()\n",
    "    the_df_dict['embeddings'] = clean_embedding_list\n",
    "    \n",
    "    return [the_df_dict, the_embeddings_np]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_dict, embeddings_np = create_embedding_df(df_data_dict, embedding_tweets, tokens_idx)\n",
    "df_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Get word and embedding for all tweets Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_words_and_embedding_from_df(the_df):\n",
    "    \n",
    "    tokens_list = the_df.iloc[0].tokens\n",
    "    embedding_list = last_hidden_states[0][0]\n",
    "    \n",
    "    final_df_dict, tokens_idx,clean_tokens_list = create_word_df(tokens_tweets)\n",
    "    final_df_dict, the_embeddings_np = create_embedding_df(final_df_dict, embedding_list, tokens_idx)\n",
    "    final_df_dict['sentence_idx'] = 0\n",
    "    \n",
    "    #for rest of each tweet/ row\n",
    "    for sentence_idx in range(1,len(the_df)):\n",
    "        # print(sentence_idx)\n",
    "        # 1. get list of tokens/words in a tweet\n",
    "        tokens_list = the_df.iloc[sentence_idx].tokens\n",
    "        \n",
    "        # 2. get list of embedding in a tweet\n",
    "        embedding_list = last_hidden_states[0][sentence_idx]\n",
    "        \n",
    "        # 3. get word df\n",
    "        df_dict, tokens_idx,clean_tokens_list = create_word_df(tokens_list)\n",
    "        df_dict['sentence_idx'] = int(sentence_idx)\n",
    "        \n",
    "        #4. add embeddings columns to the df\n",
    "        df_dict, the_embeddings_np = create_embedding_df(df_dict, embedding_list, tokens_idx)\n",
    "        \n",
    "        \n",
    "        frames = [final_df_dict, df_dict]\n",
    "\n",
    "        final_df_dict = pd.concat(frames)\n",
    "        \n",
    "        \n",
    "    return final_df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict = get_words_and_embedding_from_df(df_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict.reset_index(drop = True, inplace = True)\n",
    "df_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Double check if the embedding created is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_word.iloc[4999].tokens[52])\n",
    "\n",
    "# # get the embedding: [tensor][sentence][words][nth value]\n",
    "# last_hidden_states[0][4999][52][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Convert our embedding column to numpy array ( to prepare for clustering )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Pandas series to numpy array\n",
    "# convert first to list, then convert to array\n",
    "embeddings_np = np.array(df_data_dict.embeddings.tolist())\n",
    "embeddings_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question. There are no duplicates, its normal?\n",
    "embeddings_list = [tuple(row) for row in embeddings_np]\n",
    "embeddings_unique_np, unique_index = np.unique(embeddings_list,axis = 0, return_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_unique_np.shape)\n",
    "unique_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>There are 22,071 duplicates. Lets Remove them from df_dict<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. convert the numpy array to tupple. \n",
    "# 2. apply duplicated() to the tuple\n",
    "# 3. get the duplicated embeddings\n",
    "# 4. slice where its not duplicated\n",
    "df_data_dict = df_data_dict[~df_data_dict['embeddings'].apply(tuple).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 9. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'> We want to make sure that documents with similar sentiments are clustered together such that we can find the topics within these clusters. Before doing so, we first need to lower the dimensionality of the embeddings as many clustering algorithms handle high dimensionality poorly. <span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9.1 Reduce Dimensionality using UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\">Out of the few dimensionality reduction algorithms, UMAP is arguably the best performing as it keeps a significant portion of the high-dimensional local structure in lower dimensionality.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_unique_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "embeddings_reduced_np = umap.UMAP(n_neighbors=15,\n",
    "                       n_components=100,\n",
    "                       metric='cosine').fit_transform(embeddings_unique_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_reduced_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9.2 Clustering KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\"> After having reduced the dimensionality of the documents embeddings to 5, we can cluster the documents with HDBSCAN. HDBSCAN is a density-based algorithm that works quite well with UMAP since UMAP maintains a lot of local structure even in lower-dimensional space. Moreover, HDBSCAN does not force data points to clusters as it considers them outliers.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install hdbscan --no-build-isolation --no-binary :all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KMeans = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50)\n",
    "labels = model_KMeans.fit_predict(X=embeddings_reduced_np)\n",
    "# positive_cluster_center = model_KMeans.cluster_centers_[0]\n",
    "# negative_cluster_center = model_KMeans.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame(labels, columns =['cluster_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict['cluster_label'] = cluster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_0 = df_data_dict[df_data_dict['cluster_label'] == 0].words.reset_index(drop = True)\n",
    "dict_1 = df_data_dict[df_data_dict['cluster_label'] == 1].words.reset_index(drop = True)\n",
    "\n",
    "\n",
    "print('cluster 0 counts : ',len(dict_0))\n",
    "print('cluster 1 counts : ',len(dict_1))\n",
    "\n",
    "# lets set 1 as positive\n",
    "for i in range(len(dict_1)):\n",
    "    print(dict_1.iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets set 0 as negative\n",
    "for i in range(len(dict_0)):\n",
    "    print(dict_0.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"background-color:Teal\">- There is a mix of postiive and negative words. fever is negative while happy is positive <br>\n",
    "<span style=\"background-color:Teal\">- Subwords are separated from their original word <br><span>\n",
    "<span style=\"background-color:Teal\">- There maybe mixed up of these sentiments because BERT takes into accounts of the word contex. <br>\n",
    "<span style=\"background-color:Teal\">- We may want to try: <br> \n",
    "    1. Sentence Embedding , and just divide into two clusters <br><span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 HDBScan Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusterHDB = hdbscan.HDBSCAN(min_cluster_size=2,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(embeddings_reduced_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(np.unique(clusterHDB.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"background-color:Teal\">Question: <span><br>\n",
    "<span style=\"background-color:Teal\">- Why are there so many cluster? This is not good for sentiment clustering<span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Load BERT sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_sent = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence = df.copy()[0:3000]\n",
    "df_sentence= func.remove_end_hashtag(df_sentence)\n",
    "df_sentence.tweet = df.tweet.apply(lambda x: clean_BERT(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweet_list = []\n",
    "\n",
    "for tweet in df_sentence['tweet']:\n",
    "    encoded_tweet = model_sent.encode(tweet)\n",
    "    embedded_tweet_list.append(encoded_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence['tweet_embedding'] = np.empty((len(df_sentence),768)).tolist()\n",
    "\n",
    "df_sentence['tweet_embedding'] = embedded_tweet_list\n",
    "\n",
    "df_sentence.iloc[0].tweet_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Save datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Save df for tweets and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence.to_json('df_sentence.json')\n",
    "df_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Save df of average sentiment embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Get all positive words list\n",
    "positive_words_list = []\n",
    "for i in range(len(df_data_dict)):\n",
    "    # imagine this is positive\n",
    "    if(df_data_dict.iloc[i].cluster_label == 1):\n",
    "        positive_words_list.append(df_data_dict.iloc[i].words)\n",
    "\n",
    "#2. Get all negative words list\n",
    "negative_words_list = []\n",
    "for i in range(len(df_data_dict)):\n",
    "    # imagine this is negative\n",
    "    if(df_data_dict.iloc[i].cluster_label == 0):\n",
    "        negative_words_list.append(df_data_dict.iloc[i].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create Dictionary from our list\n",
    "# {POSITIVE: [ 'real', 'market', 'would', 'crash', ...]}\n",
    "# {NEGATIVE: [ 'work', 'remote', 'via', '##17', ...]}\n",
    "dict_data_cluster = {}\n",
    "dict_data_cluster['POSITIVE'] = positive_words_list\n",
    "dict_data_cluster['NEGATIVE'] = negative_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create Embedding Dictionary by concatenate all the words using sentence embedding\n",
    "dict_data_embed = {}\n",
    "\n",
    "for key, value_array in dict_data_cluster.items():\n",
    "    #1. Concat the defining words in the array\n",
    "    defining_words_arr = [' '.join(value_array)]\n",
    "    \n",
    "    #2. encode this array\n",
    "    defining_vector = model_sent.encode(defining_words_arr)\n",
    "    \n",
    "    #3. insert the resulting vector to our list\n",
    "    dict_data_embed[key] = defining_vector\n",
    "    print(dict_data_embed[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1. create df with two rows to store our +ve and -ve embedding\n",
    "df_data_dict = pd.DataFrame(['positive_data','negative_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. create the embedding column\n",
    "df_data_dict['embedding'] = np.empty((len(df_data_dict), 768)).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. rename column\n",
    "df_data_dict=df_data_dict.rename(columns={0:'sentiment'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Assign value to the embedding column\n",
    "df_data_dict.embedding.iloc[0] = dict_data_embed['POSITIVE'][0]\n",
    "df_data_dict.embedding.iloc[1] = dict_data_embed['NEGATIVE'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_dict.to_json('df_data_dict.json')\n",
    "df_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Create our own +ve and -ve word dictionary using gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Create two word clusters Positive and Negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_model = gensim_api.load(\"glove-wiki-gigaword-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will be using gensim in this way\n",
    "gensim_model.most_similar([\"disadvantages\",\"troublesome\",\"problematic\",\"sad\"], topn=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(list_original_similar_words, top, gensim_model):\n",
    "    list_output = list_original_similar_words\n",
    "    \n",
    "    # 1. for each word and score tuple in list of most similar\n",
    "    for word_score_tuple in gensim_model.most_similar(list_original_similar_words, topn=top):\n",
    "        word = word_score_tuple[0]\n",
    "        if(word not in stopwords.words('english')):\n",
    "            # 2. append each word to our list\n",
    "            list_output.append(word)\n",
    "    return list(set(list_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Dictionary {category:[keywords]}\n",
    "dict_clusters = {}\n",
    "\n",
    "# get 30 words cluster for each category\n",
    "dict_clusters[\"POSITIVE\"] =\\\n",
    "get_similar_words([\"productive\", \"satisfying\", \"stimulating\"], top=60, gensim_model=gensim_model)\n",
    "\n",
    "dict_clusters[\"NEGATIVE\"] =\\\n",
    "get_similar_words([\"troublesome\",\"uninspiring\",\"lonely\",\"lazy\",\"frustrating\"], top=60, gensim_model=gensim_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_clusters[\"POSITIVE\"],'\\n')\n",
    "print(dict_clusters[\"NEGATIVE\"],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Visualize Word Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try to visualize those keywords in a 2D space by applying a dimensionality reduction algorithm (i.e. TSNE). We want to make sure that the clusters are well separated from each other.\n",
    "\n",
    "Steps:\n",
    "1. Convert our words to word2vec embedding\n",
    "2. Do dimensionality reduction\n",
    "3. Create X,y for our plot\n",
    "4. plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. word embedding\n",
    "# a. for value (list of of words) in dict_clusters.values()\n",
    "# For each value in dict_cluster.values()\n",
    "\n",
    "# b. for each word in dict_value\n",
    "# add word to our list\n",
    "\n",
    "all_similar_words = [word for value in dict_clusters.values() for word in value]\n",
    "\n",
    "# c. get the embedded form from our gensim model\n",
    "X = gensim_model[all_similar_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Reduce dimension to 2 so we can plot it \n",
    "pca = manifold.TSNE(perplexity=40, n_components=2, init='pca')\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 3. Create X and Y data for our plot\n",
    "\n",
    "# 1. prepare empty data frame\n",
    "df_target = pd.DataFrame()\n",
    "\n",
    "for key,value in dict_clusters.items():\n",
    "    # get the end index to slice our X PCA\n",
    "    #      current length of data frame so far, \n",
    "    start_index = len(df_target)\n",
    "    end_index = len(df_target) + len(value)\n",
    "    \n",
    "    # create a data frame for each category\n",
    "    # assign row entries using our pca result from above\n",
    "    # get column names\n",
    "    cluster_df = pd.DataFrame(X[start_index:end_index], columns=[\"x\",\"y\"], \n",
    "                             index=value)\n",
    "    cluster_df[\"category\"] = key\n",
    "    print(cluster_df)\n",
    "    df_target = df_target.append(cluster_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.scatterplot(data=df_target, x=\"x\", y=\"y\", hue=\"category\", ax=ax)\n",
    "\n",
    "\n",
    "# ax.set(xlabel=None, ylabel=None, xticks=[], xticklabels=[], \n",
    "#        yticks=[], yticklabels=[])\n",
    "for i in range(len(df_target)):\n",
    "    ax.annotate(df_target.index[i], \n",
    "               xy=(df_target[\"x\"].iloc[i],df_target[\"y\"].iloc[i]), \n",
    "               xytext=(5,2), textcoords='offset points', \n",
    "               ha='right', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Remove useless words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.1 Remove words from dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty good cluster. Let's remove some words that are too close to each other and does not help with sentiment clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_exclude = ['quite','less','surprisingly','enormously','immensely','emotionally','depressing']\n",
    "dict_clusters['POSITIVE'] = [ elem for elem in dict_clusters['POSITIVE'] if elem not in positive_exclude]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_clusters['POSITIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_exclude = ['bit','enjoyable','incredibly','extremely','pretty','exceedingly', 'stressful']\n",
    "dict_clusters['NEGATIVE'] = [ elem for elem in dict_clusters['NEGATIVE'] if elem not in negative_exclude]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2 Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['index'] = range(1, len(df_target) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_target.reset_index().set_index('index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target=df_target.rename(columns={'level_0':'words', 'categoty':'sentiment'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_all = ['quite','less','surprisingly','enormously','immensely','emotionally', 'bit','enjoyable','incredibly','extremely','pretty','exceedingly']\n",
    "df_target = df_target[~df_target['words'].isin(exclude_all)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_target.set_index('words') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.scatterplot(data=df_target, x=\"x\", y=\"y\", hue=\"category\", ax=ax)\n",
    "\n",
    "\n",
    "# ax.set(xlabel=None, ylabel=None, xticks=[], xticklabels=[], \n",
    "#        yticks=[], yticklabels=[])\n",
    "for i in range(len(df_target)):\n",
    "    ax.annotate(df_target.index[i], \n",
    "               xy=(df_target[\"x\"].iloc[i],df_target[\"y\"].iloc[i]), \n",
    "               xytext=(5,2), textcoords='offset points', \n",
    "               ha='right', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Create average embedding from our dict_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_own_embed = {}\n",
    "\n",
    "for key, value_array in dict_clusters.items():\n",
    "    # 1. concat the defining words in the array\n",
    "    defining_words_arr = [' '.join(value_array)]\n",
    "    \n",
    "    # 2. encode this array\n",
    "    defining_vector = model_sent.encode(defining_words_arr)\n",
    "    \n",
    "    #3. insert the resulting vector to our list\n",
    "    dict_own_embed[key] = defining_vector\n",
    "    print(dict_own_embed[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_own_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5 Save the average embedding as a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1. create df with two rows to store our +ve and -ve embedding\n",
    "df_own_dict = pd.DataFrame(['positive_own','negative_own'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. create the embedding column\n",
    "df_own_dict['embedding'] = np.empty((len(df_own_dict), 768)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. rename column\n",
    "df_own_dict=df_own_dict.rename(columns={0:'sentiment'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Assign value to the embedding column\n",
    "df_own_dict.embedding.iloc[0] = dict_own_embed['POSITIVE'][0]\n",
    "df_own_dict.embedding.iloc[1] = dict_own_embed['NEGATIVE'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own_dict.to_json('df_own_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
