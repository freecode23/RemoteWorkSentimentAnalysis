{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, manifold\n",
    "\n",
    "## for pre-processing\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## for language detection\n",
    "import langdetect\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "## for w2v\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "## for bert\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "  \n",
    "\n",
    "## for predicting\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re, nltk\n",
    "import my_functions as func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweetBERT.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "func.print_tweet(df, len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unidecode import unidecode\n",
    "\n",
    "def clean_BERT(text, isSentenceEmbed = True):\n",
    "\n",
    "    ''' Pre process and convert texts to a list of words \n",
    "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
    "\n",
    "    if(isSentenceEmbed == True):\n",
    "        # sentence embedding, we need hashatag\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!-?%.\\/#'+]\", \" \", text)\n",
    "    else:\n",
    "        # word embedding remove hashtag symbol\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!?%.\\/'+]\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" plus \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tweet = df.tweet.apply(lambda x: clean_BERT(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create new df that we need to create our word dictionary \n",
    "df_word = df.copy()[0:100]\n",
    "df_word = func.remove_end_hashtag(df_word)\n",
    "df_word.tweet = df.tweet.apply(lambda x: clean_BERT(x, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "func.print_tweet(df_word,len(df_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df_word = df_word.drop_duplicates(subset=['tweet'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DistilBERT Word Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>Create word dictionary<span>\n",
    "\n",
    "https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca<br>\n",
    "    https://medium.com/analytics-vidhya/bert-word-embeddings-deep-dive-32f6214f02bf<br>\n",
    "https://dzlab.github.io/dltips/en/tensorflow/create-bert-vocab/<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load and Test DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Instantiate DistilBERT tokenizer...we use the Fast version to optimize runtime\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', pad_token='[PAD]', model_max_length = 80)\n",
    "\n",
    "# 2. get word embedder (encoder)\n",
    "model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased',output_hidden_states=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = 'Dear Line Managers Appraisal your subordinate based on their Job performance and not sentiment blood-line religious group or tribe'\n",
    "\n",
    "# 1. encode\n",
    "ids = tokenizer.encode(txt, max_length = 75,pad_to_max_length=True,add_special_tokens = True)\n",
    "print(\"\\nIDs   :\\n\", ids)\n",
    "\n",
    "# 2. tokenize with CLS and SEP\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(\"\\nTokens with special:\\n\", tokens)\n",
    "\n",
    "# 3. Display the words with their IDs.\n",
    "for tup in zip(tokens, ids ):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will convert our sentence to 1 vector of 768D and store the result vector to our list\n",
    "# embedded_text_list = []\n",
    "# for i in range(5):\n",
    "#     text = df.iloc[i].tweet\n",
    "    \n",
    "#     #ids\n",
    "#     ids = tokenizer.encode(text)\n",
    "    \n",
    "#     #tokens\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "#     #array\n",
    "#     ids_arr = np.array(ids)[None,:]\n",
    "    \n",
    "#     #embed\n",
    "#     embedding = model(ids_arr)\n",
    "    \n",
    "#     embedded_text_list.append(embedding)\n",
    "# # we have 5 samples x 768 \n",
    "\n",
    "# # total number of tweet\n",
    "# print(len(embedded_text_list))\n",
    "\n",
    "# # shape of each embedded tweet\n",
    "# print(embedded_text_list[1][0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Tokenize and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1 Encoding and Tokenize Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>We first need to get the maximum length of the tweet so we can pad it properly<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_np = df_word.tweet.apply(lambda x: tokenizer.encode(x))\n",
    "max_len = 0\n",
    "for ids in ids_np.values:\n",
    "    if len(ids) > max_len:\n",
    "        max_len = len(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'>WE want our IDs numpy to be in this format<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get ids as numpy array\n",
    "id_ = df_word.tweet.apply(lambda x: tokenizer.encode(x, add_special_tokens = True)) # Add [PAD]s\n",
    "\n",
    "idJ_np = np.array([i + [0]*(max_len-len(i)) for i in id_.values])\n",
    "\n",
    "print('\\nshape:\\n', idJ_np.shape)\n",
    "print(idJ_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_mask = np.where(idJ_np != 0, 1, 0)\n",
    "print('\\nshape:\\n',attention_mask.shape)\n",
    "print(attention_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Encoding to ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Encoding\n",
    "ids_attn = df_word.tweet.apply(lambda x: tokenizer.encode_plus(x,\n",
    "                                                        add_special_tokens = True,\n",
    "                                                        max_length =max_len,# maximum length of a sentence\n",
    "                                                        truncation=True,\n",
    "                                                        return_tensors = 'np',\n",
    "                                                        padding = 'max_length')) # Add [PAD]s\n",
    "print(ids_attn)\n",
    "# print(type(ids_attn[0]['input_ids']))   \n",
    "# print(ids_attn[0]['attention_mask'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get IDs numpy\n",
    "input_ids_list = []\n",
    "for i in range(len(ids_attn)):\n",
    "    input_ids_list.append(ids_attn[i]['input_ids'][0])\n",
    "\n",
    "    \n",
    "input_ids_np = np.array(input_ids_list)\n",
    "print(input_ids_np[0])\n",
    "print('\\ninput_ids_np.shape:\\n', input_ids_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Encoding to Attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Tokenize to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_np = df_word.tweet.apply(lambda x: tokenizer.tokenize(x,\n",
    "                                                             add_special_tokens = True,\n",
    "                                                             max_length = max_len,\n",
    "                                                             truncation=True,\n",
    "                                                             return_tensors = 'np',\n",
    "                                                             pad_to_max_length=True))\n",
    "tokens_np[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save IDs, attention_mask, and tokens as DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Save Ids as Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = input_ids_np.tolist()\n",
    "\n",
    "# # assign empty list to token ids column\n",
    "df_word['token_ids'] = np.empty((len(df_word), max_len)).tolist()\n",
    "\n",
    "# assign token ids \n",
    "df_word['token_ids'] = id_list\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Save tokens in Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df = pd.DataFrame(tokens_np)\n",
    "tokens_df.rename(columns = {'tweet': 'tweet_tokens'})\n",
    "df_word['tokens'] = tokens_df\n",
    "df_word = df_word[0:100]\n",
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedded last hidden states\n",
    "last_hidden_states = model(input_ids_np, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 7. Check our last hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](last_hidden.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'> first  [0] is the Tensor <br>\n",
    "second [0] is the tweet <br>\n",
    "third [0] is the word  <br><span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the embedding of the last word(PAD) of 1st and second sentence\n",
    "print(last_hidden_states[0][0,63,:5])\n",
    "print(last_hidden_states[0][1,63,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'> Shouldn't all padding have the same embedding? <span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 8. Test Get embedding for each word in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_idx = 0\n",
    "word_idx = 0\n",
    "tokens_tweets = df_word.iloc[sentence_idx].tokens\n",
    "embedding_tweets = last_hidden_states[0][sentence_idx]\n",
    "\n",
    "print(len(tokens_tweets))\n",
    "print(embedding_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_tweet_tokens(the_embeddings_list, the_tokens_list):\n",
    "    \n",
    "    # Part A: words df\n",
    "    the_clean_tokens_list = []\n",
    "    special_tokens_idx = []\n",
    "    merged_tokens_idx = []\n",
    "    max_len = len(the_tokens_list)\n",
    "    i = 0\n",
    "    while(i < max_len):\n",
    "        word = the_tokens_list[i]           \n",
    "        # 1. join ## words first\n",
    "        # if its not the last word, \n",
    "        if(i <= (max_len-2)):\n",
    "            print('i: ',i)\n",
    "#             print(max_len-2)\n",
    "            next_word = the_tokens_list[i+1]\n",
    "            pattern = r\"(^##)\"\n",
    "            \n",
    "            # if the next word is ##\n",
    "            if (bool(re.search(pattern,next_word))):\n",
    "                word = word + next_word[2:]\n",
    "                \n",
    "                print(word)\n",
    "#                 delete next word from list\n",
    "#                 the_tokens_list.pop(i+1)\n",
    "                max_len = len(the_tokens_list)\n",
    "                i+=1\n",
    "        i+=1\n",
    "#         # 2. remove tokens that are padding, CLS, PAD, and SEP\n",
    "#         if(word!= '[CLS]') and (the_tokens_list[i]!= '[PAD]') and (word!= '[SEP]'):\n",
    "#             the_clean_tokens_list.append(word)\n",
    "#             special_tokens_idx.append(i)\n",
    "            \n",
    "#     the_df_dict= pd.DataFrame(the_clean_tokens_list)\n",
    "#     the_df_dict.columns = ['words']\n",
    "#     return the_df_dict  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = create_df_from_tweet_tokens(embedding_tweets, tokens_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 9. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background-color:Teal'> We want to make sure that documents with similar sentiments are clustered together such that we can find the topics within these clusters. Before doing so, we first need to lower the dimensionality of the embeddings as many clustering algorithms handle high dimensionality poorly. <span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9.1 Reduce Dimensionality using UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\">Out of the few dimensionality reduction algorithms, UMAP is arguably the best performing as it keeps a significant portion of the high-dimensional local structure in lower dimensionality.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap_tweet_list = []\n",
    "for embedded_tweet in embedded_text_list:\n",
    "    umap_tweet = umap.UMAP(n_neighbors=15,\n",
    "                           n_components=5,\n",
    "                           metric='cosine').fit_transform(embedded_tweet)\n",
    "    umap_tweet_list.append(umap_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of tweet\n",
    "print(len(umap_tweet_list))\n",
    "\n",
    "# shape of each embedded tweet\n",
    "print(umap_tweet_list[6268].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9.2 Clustering KNN?\n",
    "Question : I see HDBSCAN, can I just use KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:Teal\"> After having reduced the dimensionality of the documents embeddings to 5, we can cluster the documents with HDBSCAN. HDBSCAN is a density-based algorithm that works quite well with UMAP since UMAP maintains a lot of local structure even in lower-dimensional space. Moreover, HDBSCAN does not force data points to clusters as it considers them outliers.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install hdbscan --no-build-isolation --no-binary :all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)\n",
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST BERT JALAMAAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST BERT JALAMAAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_1 = df[:100]\n",
    "batch_1[1].value_counts()\n",
    "batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "max_lenJal = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking\n",
    "attention_maskJal = np.where(padded != 0, 1, 0)\n",
    "attention_maskJal.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(padded, attention_mask=attention_maskJal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_hidden_states[0][5][53][:5])\n",
    "print(last_hidden_states[0][1][53][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_1.iloc[5][0])\n",
    "print(batch_1.iloc[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
